# 场景切换后Reward下降问题分析

## 问题现象

1. **成功率仍然不高**
2. **换了场景后reward变得特别差**

## 可能的原因

### 1. 场景难度突然增加 ⚠️ **严重**

**问题**:
- 当前场景达到80%成功率后，切换到新场景
- 新场景的起点和目标点是随机生成的
- 新场景的距离可能比旧场景大得多
- Agent还没有适应新场景，导致reward下降

**证据**:
- Reward曲线在场景切换时突然下降
- 成功率在切换后下降

### 2. 策略过拟合到旧场景 ⚠️ **严重**

**问题**:
- Agent在旧场景中学习到了特定的策略
- 这个策略可能只适用于旧场景的特定起点和目标
- 切换到新场景后，策略不适用，导致性能下降

**影响**:
- 需要重新学习新场景
- 可能导致"灾难性遗忘"

### 3. 场景切换时机不当 ⚠️ **中等**

**问题**:
- 80%成功率可能还不够稳定
- 切换时agent可能还没有完全掌握旧场景
- 导致在新场景中表现更差

### 4. 新场景距离分布问题 ⚠️ **中等**

**问题**:
- `generate_random_positions()`生成的位置可能距离差异很大
- 如果新场景距离很大（如800m），而旧场景距离较小（如300m）
- Agent需要更多时间适应

## 诊断方法

### 1. 跟踪场景切换时的episode
- 保存场景切换前后的episode
- 分析新场景的初始距离、reward等
- 观察agent如何适应新场景

### 2. 分析场景难度
- 记录每个场景的起点、目标、距离
- 分析场景难度与reward的关系
- 检查是否有难度突然增加的情况

### 3. 检查策略适应
- 观察新场景中前几个episode的表现
- 检查是否快速适应还是需要很长时间

## 可能的解决方案

### 方案1: 渐进式场景难度
- 不是完全随机生成新场景
- 根据当前场景的难度，逐步增加难度
- 例如：新场景距离 = 旧场景距离 * 1.1

### 方案2: 场景切换缓冲
- 切换后，给agent一些"适应时间"
- 在适应时间内，不计算成功率
- 或者降低成功率要求

### 方案3: 更严格的切换条件
- 提高成功率阈值（80% → 90%）
- 增加评估窗口（50 → 100 episodes）
- 确保agent真正掌握旧场景

### 方案4: 场景难度控制
- 限制新场景的距离范围
- 确保新场景不会比旧场景难太多
- 例如：新场景距离在[旧距离*0.9, 旧距离*1.2]范围内

